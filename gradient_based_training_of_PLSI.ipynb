{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_based_training_of_PLSI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8Vn5/VeigA9VXohGEBgp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-stats1/blob/main/gradient_based_training_of_PLSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 勾配法によるPLSIの学習"
      ],
      "metadata": {
        "id": "_B2H9m_9kHFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* EMアルゴリズムではなく、勾配法でPLSIの学習を行ってみる\n",
        " * 偏微分係数をゼロとおいた方程式を解く、という方法ではなく、勾配を使って、パラメータを更新する"
      ],
      "metadata": {
        "id": "d9yuXtCs49Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 例題として、ライブドア・ニュース・コーパスのトピック分析を行う"
      ],
      "metadata": {
        "id": "0zeKMFnr5M0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MeCabのインストール\n",
        "* 今回、日本語データを使うので、この作業が必要になっている"
      ],
      "metadata": {
        "id": "5fRAsKwXW3m2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN4WVrEgTDWk"
      },
      "outputs": [],
      "source": [
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "#!pip install fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "metadata": {
        "id": "eru9zCwiUVyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの取得"
      ],
      "metadata": {
        "id": "p5A1ayIITtWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ライブドア・ニュース・コーパスの前処理については下の記事を参考にした。\n",
        " * https://tech.fusic.co.jp/posts/2021-04-23-bert-multi-classification/"
      ],
      "metadata": {
        "id": "1s1d6oUlTTPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import re\n",
        "import csv\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# データのダウンロード（カレントディレクトリに圧縮ファイルがダウンロードされる）\n",
        "urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\", \"ldcc-20140209.tar.gz\")\n",
        "\n",
        "# ダウンロードした圧縮ファイルのパスを設定\n",
        "tgz_fname = \"ldcc-20140209.tar.gz\" \n",
        "\n",
        "#処理をした結果を保存するファイル名 \n",
        "tsv_fname = \"all_text.tsv\" "
      ],
      "metadata": {
        "id": "PLwMAuiTTMr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname_class_list = {\n",
        "  \"dokujo-tsushin\": [],\n",
        "  \"it-life-hack\": [],\n",
        "  \"kaden-channel\": [],\n",
        "  \"livedoor-homme\": [],\n",
        "  \"movie-enter\": [],\n",
        "  \"peachy\": [],\n",
        "  \"smax\": [],\n",
        "  \"sports-watch\": [],\n",
        "  \"topic-news\": []\n",
        "}\n",
        "target_genres = list(fname_class_list.keys())"
      ],
      "metadata": {
        "id": "07cZ5oSCTej8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 記号などを除きつつ、ニュース記事の本文を取得する"
      ],
      "metadata": {
        "id": "NQw3u5Ys51Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_brackets(inp):\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  output = re.sub(brackets_head, '', re.sub(brackets_tail, '', inp))\n",
        "  return output\n",
        "\n",
        "def read_body(f):\n",
        "  # 2行スキップ\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  next(f) # タイトル\n",
        "  lines = [line.decode('utf-8').strip() for line in f]\n",
        "  body = ' '.join(lines)\n",
        "  body = remove_brackets(body)\n",
        "  return body\n",
        "\n",
        "# all_text.tsvを作る\n",
        "with tarfile.open(tgz_fname) as tf:\n",
        "  # 対象ファイルの選定\n",
        "  for ti in tf:\n",
        "    \"\"\"\n",
        "    ・ライセンスファイルはスキップ\n",
        "    ・genre内のtxt意外ならスキップ\n",
        "    ・txtファイル意外ならスキップ\n",
        "    ・用意したgenre意外ならスキップ\n",
        "    \"\"\"\n",
        "    if \"LICENSE.txt\" in ti.name:\n",
        "      continue\n",
        "    if len(ti.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not ti.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    genre = ti.name.split('/')[1]\n",
        "    if not genre in target_genres:\n",
        "      continue\n",
        "        \n",
        "    genre_index = target_genres.index(genre)\n",
        "    fname_class_list[target_genres[genre_index]].append(ti.name)\n",
        "\n",
        "  with open(tsv_fname, \"w\") as wf:\n",
        "    writer = csv.writer(wf, delimiter='\\t')\n",
        "    for i, genre in enumerate(target_genres):\n",
        "      for fname in fname_class_list[genre]:\n",
        "        f = tf.extractfile(fname)\n",
        "        row = [genre, i, read_body(f)]\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "XkG73IllTiZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 作成したデータの読み込み\n",
        "df = pd.read_csv(\"all_text.tsv\", delimiter='\\t', header=None, names=['media_name', 'label', 'body'])\n",
        "df = df.dropna(how='any') # nanのところは落とす\n",
        "\n",
        "# データの確認\n",
        "print(f'データサイズ： {df.shape}')\n",
        "display(df.sample(10))"
      ],
      "metadata": {
        "id": "cwURoB4iTksz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トークン化\n",
        "* MeCabを使って形態素解析することで、テキストをトークン化する"
      ],
      "metadata": {
        "id": "YzceAJIVkSku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "m = MeCab.Tagger()"
      ],
      "metadata": {
        "id": "F7izFzJgTmz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.parse(df.body[0])"
      ],
      "metadata": {
        "id": "4i8PpjbjVeZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = []\n",
        "  for line in m.parse(text).splitlines():\n",
        "    fields = line.split()\n",
        "    if len(fields) != 2: continue\n",
        "    subfields = fields[1].split(',')\n",
        "    if len(subfields) != 9: continue\n",
        "    if subfields[0] in ['記号', '助詞', '助動詞', '連体詞', '副詞']: continue\n",
        "    token = subfields[6]\n",
        "    if token == '*':\n",
        "      token = fields[0]\n",
        "    tokens.append(token)\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "sudfkedfUJ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for body in df.body:\n",
        "  corpus.append(tokenize(body))"
      ],
      "metadata": {
        "id": "xheSvKt4Ublh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 文書数は7,367件"
      ],
      "metadata": {
        "id": "w9HeSp2Gfbg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "43HaBqAeXOIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=20, max_df=0.2)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "vocab = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "A_KJI4Ada7Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "s1egTMz3bH1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PLSIの学習\n",
        "* $\\log (\\mathcal{D} ) = \\sum_{d=1}^D \\log p(\\mathbf{x}_d) = \\sum_{d=1}^D \\sum_{i=1}^{N_d} \\log \\bigg( \\sum_{k=1}^K \\phi_{k,x_{d,i}} \\theta_{d,k} \\bigg)$を直接最大化する\n",
        "* 尤度の式の$\\sum_{k=1}^K \\phi_{k,x_{d,i}} \\theta_{d,k}$の部分は、文書$d$の$i$番目に単語$x_{d,i}$が出現する確率を表す。この確率は、\n",
        " * 各トピック$k$におけるその単語の出現確率$\\phi_{k,x_{d,i}}$を、\n",
        " * 文書$d$における各トピックの混合率$\\theta_{d,k}$で重み付けして加算することで、求められている。\n",
        "* 以下の二つの行列を準備する\n",
        " * $\\boldsymbol{\\Theta}$: 第$d$行、第$k$列が、文書$d$におけるトピック$k$の混合率を表す。\n",
        " * $\\boldsymbol{\\Phi}$: 第$k$行、第$w$列が、トピック$k$における単語$w$の出現確率を表す。\n",
        "* すると、PLSIは、文書$d$に単語$w$が出現する確率として、$\\boldsymbol{\\Theta}\\boldsymbol{\\Phi}$の第$d$行第$w$列の値を使うモデルだと、解釈できる。"
      ],
      "metadata": {
        "id": "O8YmbMgrkZAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PyTorchを使って実装する"
      ],
      "metadata": {
        "id": "-sjlx4ti9JYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HcHWwpCKbPCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 各ニュース記事における各単語の出現頻度をPyTorchのテンソルに変換"
      ],
      "metadata": {
        "id": "L0eQCc8E6c3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = torch.from_numpy(X.toarray()).type(torch.float32)"
      ],
      "metadata": {
        "id": "TQGR_SGHfhch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PLSIのパラメータを、微分可能なテンソルとして準備"
      ],
      "metadata": {
        "id": "57PkWwCc6lEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 20\n",
        "theta = torch.randn(len(corpus), n_components, requires_grad=True)\n",
        "phi = torch.randn(n_components, len(vocab), requires_grad=True)"
      ],
      "metadata": {
        "id": "ZczCVnkzb8Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([theta, phi], lr=1.0)"
      ],
      "metadata": {
        "id": "1JAaR2SzhUgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ミニバッチ式ではなく、バッチ式で学習を進めている\n",
        " * ミニバッチにしなくてもメモリが足りるため。"
      ],
      "metadata": {
        "id": "QpmEy57d6uAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  normalized_theta = torch.nn.functional.softmax(theta, dim=1)\n",
        "  normalized_phi = torch.nn.functional.softmax(phi, dim=1)\n",
        "  word_probs = normalized_theta @ normalized_phi\n",
        "\n",
        "  loss = - (word_freqs * word_probs.log()).sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  print(f'epoch {epoch} | loss {loss}')"
      ],
      "metadata": {
        "id": "EawUV-Tmcx26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トピック語の確認"
      ],
      "metadata": {
        "id": "qp31AlkIkbiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 各々の$k$について、$\\phi_{k,w}$が大きい上位30単語を調べる"
      ],
      "metadata": {
        "id": "6y3WCtWL65Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_topic_words = 30\n",
        "for k in range(n_components):\n",
        "  print(' '.join([vocab[idx] for idx in torch.argsort(phi[k], descending=True)[:n_topic_words]]))"
      ],
      "metadata": {
        "id": "K-VFYkfJc9SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題\n",
        "* このバッチ最適化によるパラメータ推定を、ミニバッチ最適化に書き換えてみよう。\n",
        " * 同じ方法でlossを計算して比較することで、よりlossを小さくできるか、確認してみる。\n",
        "* もしくは、授業で説明したEMアルゴリズムによる推定を実装し、今回のバッチ最適化と性能比較してみよう。\n",
        "* もちろん、使用するフレームワークはPyTorchでなくてもいいです。"
      ],
      "metadata": {
        "id": "YLZxWSQrUORp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MCPWcSMNgWTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}